{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "nZSl6G6h8XvU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "nZSl6G6h8XvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "BKDzvMsn8dZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset in details"
      ],
      "metadata": {
        "id": "sGQCwRc-dtd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "dataset = open('/content/drive/MyDrive/Persian-WikiText-1.txt', 'rb').read().decode(encoding='UTF-8')\n",
        "dataset = dataset.replace('\\n', '') #put all lines one after the other\n",
        "sample = dataset[:1000000]#since our data is too large we have to take a sample from it and work with that\n",
        "char_data = sorted(set(sample)) #get a list of unique characters of the text\n",
        "len(char_data)  #number of unique characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO1ZNM1d-Hts",
        "outputId": "c729c83f-353e-4ac3-bba6-26eac100a481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using this we are generating an index mapping for the characters in the text\n",
        "charTOindex = {u:i for i, u in enumerate(char_data)}\n",
        "indexTOchar = np.array(char_data)\n",
        "first_30 = dict(list(charTOindex.items())[:30])\n",
        "txtTOint = np.array([charTOindex[c] for c in sample])\n",
        "final_chars = tf.data.Dataset.from_tensor_slices(txtTOint)\n",
        "total_indexes = []\n",
        "for i in final_chars.take(5): total_indexes.append(i)"
      ],
      "metadata": {
        "id": "b8xDjcZRBvQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we have to split our data into batches for training\n",
        "batches = final_chars.batch(100, drop_remainder=True)"
      ],
      "metadata": {
        "id": "2aTxju7tgOCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here i define a function for splitting each data sample into target and input\n",
        "def sit(batch):\n",
        "    input_text = batch[:-1]\n",
        "    target_text = batch[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "utwNoqI8gZkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset = batches.map(sit)\n",
        "total_indexes = []\n",
        "for i in final_dataset.take(5) : total_indexes.append(i[0].numpy())\n",
        "final_dataset = final_dataset.batch(50, drop_remainder=True)\n",
        "x_train, y_train =next(iter(final_dataset))\n",
        "x_train"
      ],
      "metadata": {
        "id": "qbz_0bqhgraA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228403fc-2025-40c5-c0b8-728c5d16b0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(50, 99), dtype=int64, numpy=\n",
              "array([[167, 174, 176, ...,   0, 150, 175],\n",
              "       [163, 176, 159, ..., 192, 149, 159],\n",
              "       [  0, 149, 170, ..., 150, 195, 174],\n",
              "       ...,\n",
              "       [176,   0, 170, ..., 195, 152,   0],\n",
              "       [149, 195,   0, ...,   0, 152, 149],\n",
              "       [189, 161,   0, ..., 157,   0, 192]])>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#implement model"
      ],
      "metadata": {
        "id": "wYe0InUNiDld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_f(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "kgjIHnrwjr2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(char_data), 256),\n",
        "    tf.keras.layers.GRU(1024, return_sequences=True),\n",
        "    tf.keras.layers.Dense(224)\n",
        "])\n",
        "model.compile(optimizer='adam', loss=loss_f)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='gru_prediction_model.h5', save_weights_only=True)\n",
        "history = model.fit(final_dataset, epochs=30, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "uTuqdxYxjsZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c548f16-b539-44c0-c4ec-5bc5b2e011a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "200/200 [==============================] - 16s 66ms/step - loss: 2.8729\n",
            "Epoch 2/30\n",
            "200/200 [==============================] - 9s 46ms/step - loss: 2.3686\n",
            "Epoch 3/30\n",
            "200/200 [==============================] - 9s 46ms/step - loss: 2.1412\n",
            "Epoch 4/30\n",
            "200/200 [==============================] - 9s 47ms/step - loss: 1.9524\n",
            "Epoch 5/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 1.8113\n",
            "Epoch 6/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 1.7041\n",
            "Epoch 7/30\n",
            "200/200 [==============================] - 10s 50ms/step - loss: 1.6177\n",
            "Epoch 8/30\n",
            "200/200 [==============================] - 10s 51ms/step - loss: 1.5411\n",
            "Epoch 9/30\n",
            "200/200 [==============================] - 10s 51ms/step - loss: 1.4676\n",
            "Epoch 10/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 1.3929\n",
            "Epoch 11/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 1.3134\n",
            "Epoch 12/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 1.2282\n",
            "Epoch 13/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 1.1414\n",
            "Epoch 14/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 1.0761\n",
            "Epoch 15/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 1.0588\n",
            "Epoch 16/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 1.0433\n",
            "Epoch 17/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 1.0163\n",
            "Epoch 18/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.9866\n",
            "Epoch 19/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.9596\n",
            "Epoch 20/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.9365\n",
            "Epoch 21/30\n",
            "200/200 [==============================] - 10s 48ms/step - loss: 0.9157\n",
            "Epoch 22/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8976\n",
            "Epoch 23/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8806\n",
            "Epoch 24/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8646\n",
            "Epoch 25/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8523\n",
            "Epoch 26/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8452\n",
            "Epoch 27/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8342\n",
            "Epoch 28/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8267\n",
            "Epoch 29/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8202\n",
            "Epoch 30/30\n",
            "200/200 [==============================] - 10s 49ms/step - loss: 0.8141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('prediction_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGCXjS1XpRLb",
        "outputId": "e7ef6d2e-ae4e-4e7a-e5d7-20ce45c61e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(224, 256),\n",
        "    tf.keras.layers.GRU(1024, return_sequences=True),\n",
        "    tf.keras.layers.Dense(224)\n",
        "])\n",
        "model2.load_weights('prediction_model.h5')\n",
        "model2.build(tf.TensorShape([1, None]))\n",
        "model2.reset_states()"
      ],
      "metadata": {
        "id": "UgjK6y08vM0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_characters(base_model, char_num, starting_text):\n",
        "    input_eval = [charTOindex[s] for s in starting_text]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = [starting_text]\n",
        "    for i in range(char_num):\n",
        "        predictions = base_model.predict(input_eval, verbose=0)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predicted_ids = np.array(predictions.numpy()).argmax(axis=1).reshape(-1, 1)[-1][0]\n",
        "        message = np.append(input_eval[0].numpy(), predicted_ids)[1:]\n",
        "        input_eval = tf.expand_dims(message, 0)\n",
        "        text_generated.append(indexTOchar[predicted_ids])\n",
        "\n",
        "    for i in ''.join(text_generated).split('\\n'):\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "tDkK8ySnvX3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_characters(model2,50, \"سلام\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmbsRBHvaLh",
        "outputId": "ae626c6e-0edd-4cf4-eb95-4ed688d26d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سلامی باشد. بنام های سرزمین سوزان امروزی و فرمان به سو\n"
          ]
        }
      ]
    }
  ]
}